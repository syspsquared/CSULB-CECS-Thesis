The master image for lab loads is very important, but just as important is the method for deployment.  The worst situation a group of system administrators can get into is building each lab machine with the master image instructions.  This takes far more time than it is worth.  Instead, we define a method to copy the master image to the rest of the labs.  We do this by copying the lab image to a file and moving that file to a server that the lab workstations can access.  

In our labs, the file is moved to the printservers.  The printservers have to be set up with DHCP, TFTP, PXE, and NFS.  Specifics on this can be found in the printserver section.  The image must be copied to the NFS export directory on each printserver in each lab.  We could have a central server that all workstations access for the image.  However, this causes all workstations to compete for the bandwidth given by one switch.  This slows the process down significantly.  By deploying the image to each printserver, it allows the bandwidth being taken to be isolated within each lab subnet.  All labs can then be imaged in parallel.  

Deployment of the image requires configuring the workstations to boot from network.  The clients must have their BIOS setup so that netbooting is enabled and is first priority.  It may also be a good idea to disable other booting options since they are unnecessary for clones and could pose a security risk.  The BIOS should also be passworded to prevent changes.  Technically, the BIOS password can be reset by opening the machine and setting a jumper, but in our environment, the workstation cases are locked so that the jumper is inaccessible.  

The workstations can then be booted one by one to load a temporary network image.  It will load a bootloader first to determine what operating system to finish booting with.  The top option will likely just be to boot from the local hard disk.  Another option should be to boot from network (and there may be multiple network images).  The image to clone with is just be a variations of the distribution installer modified to be used for cloning.  It contains the clone script and any other tools for use in cloning.  Before the clone script can be run, the hard disk will need to be partitioned manually once.  This is per hard disk not per clone operation, so once it is finished, it doesn't need to be done again until hardware changes.  Once the formatting is done, the script can be run while booted from network.

This script does several things to prepare, deploy, and configure the clone image.  It takes a single argument of the last octet of the workstation's IP address.  It first enables networking if it is not already enabled.  Then it sets the machines address temporarily to the one it was given in an argument.  It then needs to set the default gateway for the lab.  This will require either a second argument to the script or separate scripts for each lab subnet.  It will then need to run filesystem tools to make a new file system and a swap area to the partitions.  It then mounts the hard drive partitions to a local mount point.  It will also need to mount the NFS export directory of the printserver so that it can access the master image.  The master image is in compressed tarball format.  Once it can reach this file via a network mount, it can untar the image onto the local hard disk.  This is the longest step.  After it is finished, two more mount commands are needed to bind the process and system directories of the new image.  After that, the bootloader, lilo, is run to allow the BIOS to see the new image.  

At this point, the system is bootable.  It is not yet configured to be a unique clone though.  The last thing the script does is set the hostname and the permanent IP address by editing the configuration files on the hard disk partitions.  On some systems, the monitor may not work with the defaults the master image had.  If so, the appropriate configuration files need to be changed to update that.  Some other configurations may need to be changed, but these vary for each distribution and release so we have no way of documenting them.  It also needs to edit the /etc/printcap file and set the printer that the workstation prints to.

This system is fairly efficient and only requires about 2 hours to clone 3 labs of 90 machines each.  The only interaction between the sysadmin and each clone is to run the clone script with the IP address of the machines.  This process could be improved though.  To do so, every workstation would need to have its MAC address copied to the printserver's DHCP table.  In addition, a copy of this table would need to be available during the netboot since the small netboot image doesn't usually have a DHCP client.  This would then prevent the need for the sysadmin to enter the address since every machine would be uniquely identified by MAC address.  This can be further automated by broadcasting wakeonlan packets from the printservers to the machines to start them.  This would prevent the need for the sysadmin to even enter the lab at all.  

All of these improvements only work to the benefit of the sysadmin though.  They do not lessen the time it takes for the labs to deploy from two hours.  To this, the biggest bottleneck, network bandwidth, would have to be addressed.  The expensive solutions is to do switch upgrades.  Currently, we use 100MB managed HP switches.  We could update them to 1GB (1000MB) switches and that would cut the time down significantly.  However, there are two inexpensive solutions.  The first is to enable QOS (Quality of Service) on our managed switches.  This would force the bandwidth to be evenly distributed across machines whereas currently one machine can take most of the bandwidth and slow all other machines down.  QOS requires managed switches to support it.   The scripts could further be timed such that the machines stagger their installs.  We could send wakeonlan packets to each workstation on a timer, allowing each machine five minutes to get high bandwidth before others start competing for it.  As more machines enter, machines that started earlier finish.  Five minutes may not be the ideal time.  That can be optimized with each successive cloning session.  These improvements may or may not work as expected because we have not tested them yet.   

